\chapter{Theory}
This chapter is going to explain theories behind the techniques used for this project. Each technique will be elaborated and how and when it is being used in the project will be explained.

%% Gustav suggestion: %%
%% This is ONLY theory and not implemenation (how we use it) %%

\section{Framework}
Image Processing can be described as an umbrella term. \citep{ip_book} writes that Image Processing comes from the general field of signal processing, and that it covers ways to segment objects of interest on a digital image. This is achieved using multiple steps. \citep{ip_book} describes them as follow. It should be noted, however, that the order of the steps sometimes change, and there might be more focus on some than others. That being said, this is a general framework that can be used as a good starting point:

\textbf{Image Acquisition}
Before anything can be processed, data needs to be captured, typically using a camera. This step is all about the setup, as well as the environment, setting, light, etc.

\textbf{Pre-processing}
Here the initial setup is completed, e.g. converting the image from color to grayscale.

\textbf{Segmentation}
To be able to work with a specific object, for instance a hand, it needs to be separated from the rest of the picture. This is done using segmentation where noise and background elements are removed, so only the object of interest is seen. Thresholding is often applied to make the object stand out, e.g. make the object appear white and the background black.

\textbf{Representation}
The object needs to be representative in an intuitive manner.

\textbf{Classification}
For the system to actually know that an object is a hand or not, it has to do some classification and compare the data to some knowledge or database. This can be done with template matching and BLOB classification.

Figure \eqref{fig:ip_framework} illustrates the framework used typically when working with Image Processing.

\begin{figure}[htbp]
\centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/imageProcessing_steps.png}
\caption{Image Processing framework. - WE SHOULD MAKE OUR OWN PICTURE LATER ON!!!!!!!! - Gustav}
\label{fig:ip_framework}
\end{figure}

%% Maybe a little to blabla and too long sentences - Gustav %%
During this project a lot of image processing has been used. The different image processing techniques are used in a combination to display exactly what we want to output. Some techniques is being used to remove noise from the picture, so the important parts gets all the focus, while others are used for making the important parts more clear or making it possible to track specific parts of the output. \\
It is all these different techniques combined that makes it possible to get a functional product, but before you can use them, you will have to know how they work and how they can be included to your project. This section is going to explain the different techniques that are being used and also why and how they are being implemented in this project.

\section{The Digital Image}
Bla bla bla metatext for The Digital Image

\subsection{8 bit system and grayscale images}
As the human eye is not able to distinguish the huge numbers of photos hitting the eye, one have decided to quantify the number of photos hitting a cell. This is often quantified as bytes (8 bits), which applies to the binary system and also the structure of memory within a computer. An image with one channel of information for each pixel in the x,y axis is called grayscale, which describes the intensity of light in that specific pixel. The value of the pixel applies to the binary system as mentioned above with the value \textbf{(2 to the power of 8)}\fixme{(Include math symbol for 2 to the power of 8)}, which is 256. One would immediately note that the values of the pixels should be 1-256, but fact is that a computer counts 0 as the first value and therefore it's from 0-255.\\
Figure \eqref{fig:ip_grayscale} illustrates the theory above.
\begin{figure}[htbp]
\centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/Grayscale.jpg}
\caption{Image illustrating the principle of a grayscale image applying to the binary system of 2 to the power of 8}
\label{fig:ip_grayscale}
\end{figure}



  
This system is used on two levels, one way it's used, is continuity of components with current of 0-5Volt and the second is indexing 
 In order to use this system on two levels, for both working with components with a digital current from 0 to 5 Volts tension and for indexing the intensity of pixel values in a picture, 

\subsection{The RGB color system}
Now that the binary system and indexing an image has been represented another aspect of digital images will be presented.\\
In addition to grayscale images consisting of one channel
 
\subsection{binary images}

\subsection{Indexing a picture (x,y axis)}

   


%% Could be a little more clear and short - Gustav %%
\section{Thresholding}
Threshold is one of the most fundamental operations in point processing \fixme{WHAT is point processing? - Gustav} and is used to make an input picture binary, i.e. either black (0) or white (255). Making an image binary means to show the image in only completely black and completely white pixel values. \\
This could be useful in programs where you need to find silhouettes - tracking a person for example - and smaller details are not as important. \\
To determine which pixels that should be completely black and which that should be completely white a threshold value is required. The threshold value can be compared to a gatekeeper that lets everyone who is 18 years old or older inside the club, but denies access to people that is 17 or younger. Using this analogy, pixels that are "old" (bright) enough are let in, while "younger" pixels (dark) are denied

Using the same analogy, somebody decided that the border between being too young and just old enough should be 18 years. This age could easily be something different, such as 17 or 24. The same is true with thresholding: you have to decide when pixels are "old" enough (bright). If they are too young (dark), they cannot get access.

In practice this means that pixels with values greater than a threshold is set to TRUE (or white), which typically is 255 when talking in bytes. On the other hand, if a pixel is less than the threshold value, it is set to FALSE (black) or 0. To sum this up the formula for making a threshold is shown in equation \ref{threshold}.
\begin{equation}
  \begin{aligned}
  	\text{if } f(x,y)\leq T \quad \text{then } g(x,y)&= 0 \\
  	\text{if } f(x,y)>T \quad \text{then } g(x,y)&= 255
	\label{threshold}  
  \end{aligned} 
\end{equation}
Where $T$ is the threshold value, $f(x,y)$ is the input pixel and $g(x,y)$ is the output pixel. 

When choosing the threshold value it is important to think about what the wanted output is. It differs  from image to image how effective thresholding is based on the difference between the object and the background. If the background and the object you want to find is very different in color, then it is easy to distinguish between them and choose an effective threshold value. However, if the object and background are very similar, then it becomes harder to do choose a threshold value, since you will have to chose between losing some information from the wanted object in order to remove background noise, or keep the object clear but have more background noise. \\
To get a better understanding of this, look at the histograms shown in \eqref{fig:SimpleThreshold} and \eqref{fig:ComplicatedThreshold} that shows to grayscale pictures.

It is easy to tell that the leftmost histogram is more ideal to threshold because the object and background are very different, while the rightmost histogram has very similar object and backgrounds.

\begin{figure}[htbp] \centering
\begin{minipage}[b]{0.45\textwidth} \centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/SimpleThresholdPicture} % Venstre billede
\end{minipage} \hfill
\begin{minipage}[b]{0.45\textwidth} \centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/ComplicatedThresholdPicture} % Højre billede
\end{minipage} \\ % Captions og labels
\begin{minipage}[t]{0.45\textwidth}
\caption{Simple threshold value} % Venstre caption og label
\label{fig:SimpleThreshold}
\end{minipage} \hfill
\begin{minipage}[t]{0.45\textwidth}
\caption{Complicated threshold value} % Højre caption og label
\label{fig:ComplicatedThreshold}
\end{minipage}
\end{figure}

Looking at picture \eqref{fig:SimpleThresholdAfter} will show that the picture with the leftmost histogram, figure \eqref{fig:SimpleThreshold}, gives a clear outline of the silhouette of a woman after thresholding is applied. However, looking at figure \eqref{fig:ComplicatedThresholdAfter} shows that the picture with the rightmost histogram, figure \eqref{fig:ComplicatedThreshold}, gives a very poor silhouette of a woman because the background and the object has very similar colors. 

\begin{figure}[htbp] \centering
\begin{minipage}[b]{0.45\textwidth} \centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/SimpleThresholdAfter} % Venstre billede
\end{minipage} \hfill
\begin{minipage}[b]{0.45\textwidth} \centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/ComplicatedThresholdAfter} % Højre billede
\end{minipage} \\ % Captions og labels
\begin{minipage}[t]{0.45\textwidth}
\caption{Great silhouette after threshold} % Venstre caption og label
\label{fig:SimpleThresholdAfter}
\end{minipage} \hfill
\begin{minipage}[t]{0.45\textwidth}
\caption{Bad silhouette after threshold} % Højre caption og label
\label{fig:ComplicatedThresholdAfter}
\end{minipage}
\end{figure}
 
This proves the point that finding a functional threshold value is not always simple, and for finding a silhouette for example it is a great help to somehow make a clear transition from the object and the background.

\section{Morphology}
\subsection{Hit}
\subsection{Fit}
\subsection{Dilation}
\subsection{Erosion}
\subsection{Opening}
\section{Modified infrared camera vs normal webcam}
\section{Framework}
\section{Region of interest}
The Region of Interest or just ROI is a great image processing tool to optimize the frame rate of a camera. Cameras are getting improved all the time, and the quality and amount of pixels available increases all the time. \\
A common thinking is that the more pixels, the better quality, and the better output. However this should be thought through. How important is the quality? Is it even necessary to show every single pixel or is it just as good with a lower number of pixels?\\
When the computer make changes to a picture it has to run through every single pixel one at a time. Therefore it would make a big difference to decrease the amount of pixels used.\\
Beside thinking about the amount of pixels used a great tool is ROI. As the name alludes ROI takes a region in the picture that is specially interesting. This region is usually enclosed of a rectangle, and then only the pixels inside this rectangle is being processed. All the pixels outside of the rectangle is being ignored, and unnecessary processing is prevented, saving time and possibly providing a faster frame rate.

An example of how this could be used is taking a photo where only the head is interesting. The head should then be the region of interest, and only the pixels inside the rectangle that illustrates the region of interest will then be processed. This can be shown on figure \eqref{fig:Region of Interest}. This figure illustrate how many pixels that can be ignored, and gives a great overview of how much wasted energy that can be saved.


\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.5\textwidth]{Pictures/Theory/RegionOfInterest.jpg} 
\caption{Region of Interest} 
\label{fig:Region of Interest} 
\end{figure} 

\section{Background subtraction}
\section{Template matching}
\section{movement tracking (ViBe)}
\section{Digital image}
\section{Noise filter(median/mean)}
\section{BLOB-Analysis}
A common problem when dealing with images is to determine if the image data contains a particular object or shape. The term BLOB stands for Binary Large Objects and refers to a region of connected pixels in a binary image. This technique is used to extract meaningful information from images, separate the pixels by detecting the points or regions that differ in properties like brightness or color changes (i.e., their value), and classify them into two categories: the foreground (pixels with a non-zero value) and the background (pixels with a zero value).
Therefore, BLOB analysis will be split in three main steps: \textit{Extraction} of the BLOBs, \textit{representation} of the BLOBs and lastly, \textit{classification} of the BLOBs to know which ones belong to the expected type.
\subsection{BLOB Extraction}
To isolate BLOBs in a binary image, we need to define first if two pixels are connected or not. This is done by applying algorithms that will help to determine the connectivity of the pixels, but also the number of BLOBs contained in an image. The most common used connectivities in BLOB extraction are the 8-connectivity and the 4-connectivity kernels. Whereas the 8-connectivity kernel is more accurate, it also requires more computations and consequently, needs more time to process the image.

%% add picture here

\subsubsection{Grass-Fire Algorithm}
One of these connected component labeling algorithm is the \textbf{Recursive Grass-Fire Algorithm}, used to erode images but also to track the pixels locations to create a region.
To explain this algorithm, we will use both 8-connectivity and 4-connectivity kernels to illustrate how these choices can affect to the final result.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{Pictures/Theory/8connec_kernel.png}
\caption{8 connectivity labeling kernel detects 1 single object}
\label{fig:8connecK}
\end{figure}

As we can see on figure \eqref{fig:8connecK}, the Grass-Fire algorithm scans the whole image from the upper-left side to the right bottom, row by row. When the kernel finds a non-zero pixel value it centers its attention on it and burns it. Then the algorithm looks in the possible directions around that pixel to check if one of those pixels is connected to the previous one, and therefore, is an object. The way the algorithm will do this will depend on the connectivity used (for a 8-connectivity kernel, the algorithm will look into 8 directions, but for the 4-connectivity kernel, the algorithm will look in 4 directions).
Whenever it finds an object, the algorithm labels the pixel on the output image and then burns that pixel on the input image in order to turn it into a zero and ensure that the pixel will not be part of a new grassfire.
The algorithm looks now on the possible directions around that pixel, starting with the pixel on the right (see kernel picture \eqref{fig:8connecK}). Once it finds a non-zero pixel value, the algorithm centers its attention on it and restarts the process again labeling the pixel with the number of the previous one. At the end of that row the algorithm will check the surrounding pixels on the row below. If a non-zero pixel value is found, the algorithm will continue the process of burning and labeling pixels, otherwise the algorithm will starts its way back to the beginning checking one by one surrounding pixels to verify that it has checked all the possible directions and non-zero pixels values.
Comparing the figure \eqref{fig:8connecK} and the figure \eqref{fig:4connecK}, we can realize how the choice of a certain connectivity kernel will affect the final result of the algorithm using a same picture. Although the 8-connectivity kernel is more accurate it seems to be unable to separate the different objects correctly, whereas the 4-connectivity kernel finds the different objects performing less operations.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{Pictures/Theory/4connec_kernel.png}
\caption{4 connectivity labeling kernel detects 2 objects}
\label{fig:4connecK}
\end{figure}



Figure \eqref{fig:4connecK}

\subsection{BLOB Representation}
The classification of the BLOBs is made by creating a \textit{prototype model} of the object that we are looking for, in order to state the features that it should accomplish, and the deviations that would be acceptable. This process consist of two steps: determining the features of the BLOB and matching the features with the objects found to determine if they are part of the type that we were looking for.

Hence a few features like the \textbf{area}, the \textbf{perimeter} and the \textbf{circularity} will be calculated.
\subsubsection{The features}

\begin{itemize}
\item Area

The importance of calculating features like the \textbf{area} of a BLOB is well understood if we keep in mind that one of the first steps while classifying the detected objects is to delete those that are bigger or smaller than the expected ones. One way of doing this is by calculating the area of the BLOBs.

\item Bounding box and Bounding circle

Both \textbf{bounding box} and \textbf{bounding circle} are features that operate in a similar way, as the particularity of these features is to enclose those objects that fit into a specific box or circle. The difference between these two besides the shape, is the way it operates. While the \textbf{bounding box} is the minimum rectangle and it checks the four pixels with the minimum and maximum x and y values, the \textbf{bounding circle} is the minimum circle and needs to find first the center of the BLOB.

\item Convex hull

It's the minimum convex polygon which contains the BLOB. Starting from the topmost pixel of the BLOB and searching on the right along a horizontal line and in the clockwise angle till, it finds a BLOB pixel and creates the first side of the polygon.

\item Bounding box ratio
This is the height of the bounding box divided by the width, indicating in this way the lengthening of the BLOB.
\item Compactness
The compactness of a BLOB is the ratio of the BLOB's area to the ratio of the bounding box.

\begin{displaymath}
\center
$Compactness=\frac{BLOB's Area}{width \cdot height}
\end{displaymath}

\item Center of mass
In physics, the center of mass is the unique point where the weighted relative position of the distributed mass sums to zero. The common example to explain this would be the place where you have to place your finger on an object in order to have it balanced.

The distribution of mass is balanced around the center of mass and the average of the weighted position coordinates of the distributed mass defines its coordinates. On a binary image, this center will be the average x and y positions of the object on the image.

Its coordinates can be found using the formula:

\begin{displaymath}
\center
X_{c}=\frac{1}{N} \sum_{i=1}^{N} x_i
\end{displaymath}

\item Center of the bounding box
\item Perimeter
\item Circularity
\end{itemize}




\subsection{BLOB Classification}