\chapter{Theory}
This chapter is going to explain theories behind the techniques used for this project. Each technique will be elaborated and how and when it is being used in the project will be explained.

%% Gustav suggestion: %%
%% This is ONLY theory and not implemenation (how we use it) %%

\section{Framework}
Image Processing can be described as an umbrella term. \citep{ip_book} writes that Image Processing comes from the general field of signal processing, and that it covers ways to segment objects of interest on a digital image. This is achieved using multiple steps. \citep{ip_book} describes them as follow. It should be noted, however, that the order of the steps sometimes change, and there might be more focus on some than others. That being said, this is a general framework that can be used as a good starting point:

\textbf{Image Acquisition}
Before anything can be processed, data needs to be captured, typically using a camera. This step is all about the setup, as well as the environment, setting, light, etc.

\textbf{Pre-processing}
Here the initial setup is completed, e.g. converting the image from color to grayscale.

\textbf{Segmentation}
To be able to work with a specific object, for instance a hand, it needs to be separated from the rest of the picture. This is done using segmentation where noise and background elements are removed, so only the object of interest is seen. Thresholding is often applied to make the object stand out, e.g. make the object appear white and the background black.

\textbf{Representation}
The object needs to be representative in an intuitive manner.

\textbf{Classification}
For the system to actually know that an object is a hand or not, it has to do some classification and compare the data to some knowledge or database. This can be done with template matching and BLOB classification.

Figure \eqref{fig:ip_framework} illustrates the framework used typically when working with Image Processing.

\begin{figure}[htbp]
\centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/imageProcessing_steps.png}
\caption{Image Processing framework. - WE SHOULD MAKE OUR OWN PICTURE LATER ON!!!!!!!! - Gustav}
\label{fig:ip_framework}
\end{figure}

%% Maybe a little to blabla and too long sentences - Gustav %%
During this project a lot of image processing has been used. The different image processing techniques are used in a combination to display exactly what we want to output. Some techniques is being used to remove noise from the picture, so the important parts gets all the focus, while others are used for making the important parts more clear or making it possible to track specific parts of the output. \\
It is all these different techniques combined that makes it possible to get a functional product, but before you can use them, you will have to know how they work and how they can be included to your project. This section is going to explain the different techniques that are being used and also why and how they are being implemented in this project.

\section{The Digital Image}
Bla bla bla metatext for The Digital Image
\begin{figure}[htbp]
\centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/ColoredToGrayscaleToBinary.jpg}
\caption{Image illustrating the principle of an 8 bit picture}
\label{fig:ColoredToGrayscaleToBinary}
\end{figure}


\subsection{8 bit Pictures and Grayscale Images}
As the human eye is not able to distinguish the huge numbers of photos hitting the eye, one have decided to quantify the number of photos hitting a cell. This is often quantified as bytes (8 bits), which applies to the binary system and also the structure of memory within a computer.\\
An image with one channel of information for each pixel in the x and y axis is called grayscale image, which describes the intensity of light in that specific pixel. The value of the pixel applies to the binary system with the value of $2^8$, which is 256. That means by having an 8-bit picture with 256 levels of intensity, it is possible for the computer to distinguish 256 different levels of one particular color, giving more depth. In addition one would note that the values of the pixels should be 1-256, but fact is that a computer counts 0 as the first value and therefore it is from 0-255. An 8-bit picture is a pretty standard format for a picture, but it is however possible to create images with more depth and with more information for each individual pixel in the picture.\\

\begin{figure}[htbp]
\centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/Grayscale.jpg}
\caption{Image illustrating the principle of an 8 bit picture}
\label{fig:ip_grayscale}
\end{figure}
Figure \eqref{fig:ip_grayscale} Illustrates 256 different levels of grayscale.\\

An example of that is 16, and 32 bit pictures. An 16 bit picture is equals to $2^{16}$, which means that each pixel has 65,536 different shades of gray. Simultaneously a 32 bit is equal to $2^{32}$, which means that the picture has 4,294,967,296 different shades of gray. It should however be notified that working with 8 bit picture is the most standard format when doing computer operations as it consumes a lot of process power to calculate the information in each individual pixel.
 
\subsection{Working with Colored Images (RGB)}
Now that a basic knowledge regarding images has been established, the next leap into using images in calculations is by understanding the basics of a colored image.\\
Figure \eqref{fig:ip_ColorWheel} Illustrates the making of colors in relation to the RGB system.\\
\begin{figure}[htbp]
\centering
\includegraphics[width=0.70\textwidth]{Pictures/Theory/RGBColor.pdf}
\caption{Image illustrating the making of colors from http://www.texample.net/tikz/examples/rgb-color-mixing/}
\label{fig:ip_ColorWheel}
\end{figure}\\
Different from a grayscale image, a colored image consists of 3 channels of colors that each describe the value of the color in relation to red, blue or green, which are the primary colors from which all other colors are derived. The most significant different between grayscale images and colored images, is that now one would not describe the intensity of one specific color, but a mix of the red, green and blue color. In addition giving the red, green and blue pixel a value of 255 will produce white, while giving them all the value 0 would produce black, just as when working with grayscale images.\\
Knowing the specific values of the red, green and blue channels within an image gives the user a great advantage, especially when performing image manipulations in programming. If one wants to exclude a specific color in a mathematical operation, such as deriving colors adjoining to red from the input, one would simply have to create a threshold to segment the the minimum and maximum values of the red channel. The outcome should be a picture with a limited/controlled amount of red. 

\subsection{Binary Images}
One edge of image operations is binary pictures. In comparison to above-mentioned that is all about picture depth and pixel information, binary pictures is represented by two colors only in form of black and white. Using this indicates the use of a threshold, described in section \ref{sec:Thresholding}.\\
Using the tool correctly, one will be able to adjust the input so that only usable pixel information remains in the picture. However one should be careful when using thresholding, as it's easy to over- and undersegmentate an input.\\
Binary pictures are less resource demanding when performing mathematical operations as only two colors remain of the original input. In 

\subsection{Indexing a picture (x,y axis)}

   


%% Could be a little more clear and short - Gustav %%
\section{Thresholding}
\label{sec:Thresholding}
Threshold is one of the most fundamental operations in point processing \fixme{WHAT is point processing? - Gustav} and is used to make an input picture binary, i.e. either black (0) or white (255). Making an image binary means to show the image in only completely black and completely white pixel values. \\
This could be useful in programs where you need to find silhouettes - tracking a person for example - and smaller details are not as important. \\
To determine which pixels that should be completely black and which that should be completely white a threshold value is required. The threshold value can be compared to a gatekeeper that lets everyone who is 18 years old or older inside the club, but denies access to people that is 17 or younger. Using this analogy, pixels that are "old" (bright) enough are let in, while "younger" pixels (dark) are denied

Using the same analogy, somebody decided that the border between being too young and just old enough should be 18 years. This age could easily be something different, such as 17 or 24. The same is true with thresholding: you have to decide when pixels are "old" enough (bright). If they are too young (dark), they cannot get access.

In practice this means that pixels with values greater than a threshold is set to TRUE (or white), which typically is 255 when talking in bytes. On the other hand, if a pixel is less than the threshold value, it is set to FALSE (black) or 0. To sum this up the formula for making a threshold is shown in equation \ref{threshold}.
\begin{equation}
  \begin{aligned}
  	\text{if } f(x,y)\leq T \quad \text{then } g(x,y)&= 0 \\
  	\text{if } f(x,y)>T \quad \text{then } g(x,y)&= 255
	\label{threshold}  
  \end{aligned} 
\end{equation}
Where $T$ is the threshold value, $f(x,y)$ is the input pixel and $g(x,y)$ is the output pixel. 

When choosing the threshold value it is important to think about what the wanted output is. It differs  from image to image how effective thresholding is based on the difference between the object and the background. If the background and the object you want to find is very different in color, then it is easy to distinguish between them and choose an effective threshold value. However, if the object and background are very similar, then it becomes harder to do choose a threshold value, since you will have to chose between losing some information from the wanted object in order to remove background noise, or keep the object clear but have more background noise. \\
To get a better understanding of this, look at the histograms shown in \eqref{fig:SimpleThreshold} and \eqref{fig:ComplicatedThreshold} that shows to grayscale pictures.

It is easy to tell that the leftmost histogram is more ideal to threshold because the object and background are very different, while the rightmost histogram has very similar object and backgrounds.

\begin{figure}[htbp] \centering
\begin{minipage}[b]{0.45\textwidth} \centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/SimpleThresholdPicture} % Venstre billede
\end{minipage} \hfill
\begin{minipage}[b]{0.45\textwidth} \centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/ComplicatedThresholdPicture} % Højre billede
\end{minipage} \\ % Captions og labels
\begin{minipage}[t]{0.45\textwidth}
\caption{Simple threshold value} % Venstre caption og label
\label{fig:SimpleThreshold}
\end{minipage} \hfill
\begin{minipage}[t]{0.45\textwidth}
\caption{Complicated threshold value} % Højre caption og label
\label{fig:ComplicatedThreshold}
\end{minipage}
\end{figure}

Looking at picture \eqref{fig:SimpleThresholdAfter} will show that the picture with the leftmost histogram, figure \eqref{fig:SimpleThreshold}, gives a clear outline of the silhouette of a woman after thresholding is applied. However, looking at figure \eqref{fig:ComplicatedThresholdAfter} shows that the picture with the rightmost histogram, figure \eqref{fig:ComplicatedThreshold}, gives a very poor silhouette of a woman because the background and the object has very similar colors. 

\begin{figure}[htbp] \centering
\begin{minipage}[b]{0.45\textwidth} \centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/SimpleThresholdAfter} % Venstre billede
\end{minipage} \hfill
\begin{minipage}[b]{0.45\textwidth} \centering
\includegraphics[width=1.00\textwidth]{Pictures/Theory/ComplicatedThresholdAfter} % Højre billede
\end{minipage} \\ % Captions og labels
\begin{minipage}[t]{0.45\textwidth}
\caption{Great silhouette after threshold} % Venstre caption og label
\label{fig:SimpleThresholdAfter}
\end{minipage} \hfill
\begin{minipage}[t]{0.45\textwidth}
\caption{Bad silhouette after threshold} % Højre caption og label
\label{fig:ComplicatedThresholdAfter}
\end{minipage}
\end{figure}
 
This proves the point that finding a functional threshold value is not always simple, and for finding a silhouette for example it is a great help to somehow make a clear transition from the object and the background.

\section{Morphology}
\subsection{Hit}
\subsection{Fit}
\subsection{Dilation}
\subsection{Erosion}
\subsection{Opening}
\section{Modified infrared camera vs normal webcam}
\section{Framework}
\section{Region of interest}
The Region of Interest or just ROI is a great image processing tool to optimize the frame rate per second for the output. Cameras are getting improved all the time, and the quality and amount of pixels available increases all the time. \\
A common thinking is that the more pixels, the better quality, and the better output. However this should be reconsidered. One should think about different aspects like; How important is the quality and if it necessary to show every single pixel or if it is satisfying with a lower number of pixels.\\
When the computer make changes to a picture it has to run through every single pixel one at a time. Therefore it would make a big difference to decrease the amount of pixels used.\\
Beside thinking about the amount of pixels used, a great tool is ROI. As the name alludes ROI takes a region in the picture that is especially interesting. This region is usually enclosed of a rectangle, and then only the pixels inside this rectangle is being processed. All the pixels outside of the rectangle is being ignored, and unnecessary processing power is prevented, saving time and possibly providing a faster frame rate on the output.

An example of how this could be used, is taking a photo where only the head is interesting. The head should then be the region of interest and only the pixels inside the rectangle that illustrates the region of interest will then be processed. This is illustrated on figure \eqref{fig:Region of Interest}. This figure illustrate how many pixels that can be ignored, and gives a great overview of how much wasted processing power that can be saved.


\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.5\textwidth]{Pictures/Theory/RegionOfInterest.jpg} 
\caption{Region of Interest} 
\label{fig:Region of Interest} 
\end{figure} 

\section{Background subtraction}
%% Working progress - Johannes %%
A way to detect changes in a scene and extracting an object is using background subtraction. As the name tells it is done by subtracting the background from the scene so only changes is being shown. Background subtraction is very efficient but unfortunately it is not always as simple as it sounds. \\
To be able to use background subtraction efficiently a controlled setup is required. The optimal setup is indoor with controllable light conditions. The setup is important because the background should not change. Imagine if sunshine is let inside a room. Then the light conditions will change together with the suns position, and changes will be seen everywhere spreading noise and giving an inaccurate result. \\
However, it is not realistic that each pixel in the background keeps the exact same pixel value all the time. Therefore a threshold value is required - for two reasons. First of all to make the changes in the scene more distinct from the background, but also to have a threshold value in which the background may vary. \\
Therefore the following two points has to be considered when doing background subtraction:
\begin{enumerate} 
	\item Is the background consistent? 
	\item Which threshold value would be optimal for making the picture binary, and distinct the changes from the background? 
\end{enumerate}

If the first point is false, and the background is not consistent then there is a way to automatically update the backgrounds pixel values. The formula is as following:

\begin{equation}
	\begin{aligned}
  		r_{new}(x,y)=\alpha*r_{old}(x,y)+(1-\alpha)*f(x,y)
		\label{AutomaticBackground}  
 	\end{aligned}
\end{equation}  

Where $r(x,y)$ is the reference image, $f(x,y)$ is the current image, and $\alpha$ is the weighting factor. The weighting factor $\alpha$ defines how fast the reference image updates. A typical weighting factor value is  $\alpha = 0.95$
\fixme{Insert reference - formula taken from page 70 in the Image Processing book}
Now that the background automatically updates, it is time to go to the next point.


If the background is consistent from the beginning then it also leads to the next point. 

The second point is about defining a threshold value. \\
To do this, it is necessary to check how much the backgrounds pixel values vary. This can be done by making a histogram for some random pixels in the background and check the values after recording some minutes. This should give an overview of how much the pixel values vary. Lets say that this procedure shows that an efficient threshold value is 20. This means that if a pixel in the background is of the intensity 80, then pixel values captured between the values $60-100$ will not make any changes, as it is thought as being the background. Pixels below 60 and above 100 will however be an object and be highlighted.
Another pixel in the background could have the intensity of 48 and the pixel values captured between $28-68$ would then be thought of as the background while values below 28 and above 68 would be seen as an object and be highlighted. \\
This type of threshold is a global threshold, which means that the threshold value is the same in every pixel in the scene. Imagine a scene where only part of the picture is being affected by lights. Then a unique threshold value for each pixel could be useful. This is possible using the following formula:

\begin{equation}
	\begin{aligned}
  		\ \text{Binary image} = \left\{ \begin{array}{ll}
         0, \quad &\text{if } Abs(g(x,y))<\beta * \sigma(x,y)\\
        255, \quad &\text{otherwise}.
        \end{array} \right . \ 
 	\end{aligned}
\end{equation} 

Where $\beta$ is the scaling factor, and $\sigma(x,y)$ is the standard deviation at the position $(x,y)$. Using this formula creates a local threshold.

When both points are considered, the background subtraction should be ready to use. In every frame in a scene each pixel will be checked for a possible change. If there is a change larger than what the threshold value allows then the pixel will be highlighted, revealing an object in the picture.
\section{Template matching}
\section{movement tracking (ViBe)}
\section{Digital image}
\section{Noise filter(median/mean)}
\section{BLOB-Analysis}
A common problem when dealing with images is to determine if the image data contains a particular object or shape. The term BLOB stands for Binary Large Objects and refers to a region of connected pixels in a binary image. This technique is used to extract meaningful information from images, separate the pixels by detecting the points or regions that differ in properties like brightness or color changes (i.e., their value), and classify them into two categories: the foreground (pixels with a non-zero value) and the background (pixels with a zero value).
Therefore, BLOB analysis will be split in three main steps: \textit{Extraction} of the BLOBs, \textit{representation} of the BLOBs and lastly, \textit{classification} of the BLOBs to know which ones belong to the expected type.
\subsection{BLOB Extraction}
To isolate BLOBs in a binary image, we need to define first if two pixels are connected or not. This is done by applying algorithms that will help to determine the connectivity of the pixels, but also the number of BLOBs contained in an image. The most common used connectivities in BLOB extraction are the 8-connectivity and the 4-connectivity kernels. Whereas the 8-connectivity kernel is more accurate, it also requires more computations and consequently, needs more time to process the image.

%% add picture here

\subsubsection{Grass-Fire Algorithm}
One of these connected component labeling algorithm is the \textbf{Recursive Grass-Fire Algorithm}, used to erode images but also to track the pixels locations to create a region.
To explain this algorithm, we will use both 8-connectivity and 4-connectivity kernels to illustrate how these choices can affect to the final result.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{Pictures/Theory/8connec_kernel.png}
\caption{8 connectivity labeling kernel detects 1 single object}
\label{fig:8connecK}
\end{figure}

As we can see on figure \eqref{fig:8connecK}, the Grass-Fire algorithm scans the whole image from the upper-left side to the right bottom, row by row. When the kernel finds a non-zero pixel value it center its attention on it and burns it.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{Pictures/Theory/4connec_kernel.png}
\caption{4 connectivity labeling kernel detects 2 objects}
\label{fig:4connecK}
\end{figure}



Figure \eqref{fig:4connecK}

\subsection{BLOB Classification}
The classification of the BLOBs is made by creating a \textit{prototype model} of the object that we are looking for, in order to state the features that it should accomplish, and the deviation that would be acceptable.


